{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear all variables\n",
    "for i in list(globals().keys()):\n",
    "    if(i[0] != '_'):\n",
    "        exec('del {}'.format(i))\n",
    "\n",
    "#suppress future warnings -- not really a good idea \n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#%%  # clear all variables\n",
    "#clear all variables\n",
    "for i in list(globals().keys()):\n",
    "    if(i[0] != '_'):\n",
    "        exec('del {}'.format(i))\n",
    "        \n",
    "# suppress future warnings -- not really a good idea \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import libraries/dependencies\n",
    "import csv\n",
    "import xlrd\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib import figure\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf') # uses vector figures in pdf exports --\n",
    "plt.style.use('seaborn-pastel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions.. these will end up in a 'utils.py'\n",
    "def load_data (fName):\n",
    "    data = pd.read_csv(fName,header = 0)\n",
    "    print('Data: ', data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# remove missing values with a given threshold\n",
    "def remove_missing_values(data, nanThreshold):\n",
    "    #1: remove  NaN: #  drop features that have over nanThreshold% missing values\n",
    "    df=data.dropna(axis=1, thresh=int(nanThreshold*len(data)), subset=None, inplace=False)\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    print('before {} after {}'.format(data.shape, df.shape))   \n",
    "    \n",
    "    return df\n",
    "\n",
    "# replace with LabenEncoder()\n",
    "# encode features \n",
    "def encode_features_v2(df):\n",
    "    # sex\n",
    "    df=df.replace({'female':'0'}) \n",
    "    df=df.replace({'male':'1'})\n",
    "  \n",
    "    #drug_exposure\n",
    "    df=df.replace({'No':'0'})\n",
    "    df=df.replace({'Yes':'1'})\n",
    "   \n",
    "    #vr.stringent; vr.lenient\n",
    "    df=df.replace({'non-responder':'0'})\n",
    "    df=df.replace({'vasoresponder':'1'})\n",
    "\n",
    "    # ec_right_ventricle\n",
    "    df=df.replace({'normal':'0'})\n",
    "    df=df.replace({'abnormal':'1'})\n",
    "\n",
    "    # img_emphysema_category; img_fibrosis_category    \n",
    "    df=df.replace({'none':'0'})\n",
    "    df=df.replace({'minimal/mild':'1'})\n",
    "    df=df.replace({'moderate':'2'})\n",
    "    df=df.replace({'severe':'3'})\n",
    "\n",
    "    # img_thromboembolic_disease\n",
    "    #df=df.replace({'UNK':'NA'})\n",
    "    df=df.replace({'no':'0'})\n",
    "    df=df.replace({'yes':'1'})\n",
    "\n",
    "    # smoking_habit\n",
    "    df=df.replace({'no':'0'})\n",
    "    df=df.replace({'past/current':'1'})\n",
    "    \n",
    "    # reveal_risk\n",
    "    df=df.replace({'low risk':'0'})\n",
    "    df=df.replace({'moderately high risk':'1'})\n",
    "    df=df.replace({'high risk':'2'})\n",
    "    df=df.replace({'very high risk':'3'})\n",
    "    \n",
    "    # initial_therapy\n",
    "    df=df.replace({'PDE5':'0'})\n",
    "    df=df.replace({'CCB':'1'})\n",
    "    df=df.replace({'ERA':'2'})\n",
    "    df=df.replace({'PA':'3'})\n",
    "    df=df.replace({'sGC':'4'})\n",
    "    df=df.replace({'combination therapy':'5'})\n",
    "    \n",
    "    #UNK -- NA\n",
    "    # df=df.replace({'UNK':''})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# encode features \n",
    "def encode_features_v3(df):\n",
    "    # sex\n",
    "    df=df.replace({'female':'0'}) \n",
    "    df=df.replace({'male':'1'})\n",
    "  \n",
    "    #drug_exposure\n",
    "    df=df.replace({'No':'0'})\n",
    "    df=df.replace({'Yes':'1'})\n",
    "   \n",
    "    #vr.stringent; vr.lenient\n",
    "    df=df.replace({'non-responder':'0'})\n",
    "    df=df.replace({'vasoresponder':'1'})\n",
    "\n",
    "    # ec_right_ventricle\n",
    "    df=df.replace({'normal':'0'})\n",
    "    df=df.replace({'abnormal':'1'})\n",
    "\n",
    "    # img_emphysema_category; img_fibrosis_category    \n",
    "    df=df.replace({'none':'0'})\n",
    "    df=df.replace({'minimal/mild':'1'})\n",
    "    df=df.replace({'moderate':'2'})\n",
    "    df=df.replace({'severe':'3'})\n",
    "\n",
    "    # img_thromboembolic_disease\n",
    "    #df=df.replace({'UNK':'NA'})\n",
    "    df=df.replace({'no':'0'})\n",
    "    df=df.replace({'yes':'1'})\n",
    "\n",
    "    # smoking_habit\n",
    "    df=df.replace({'no':'0'})\n",
    "    df=df.replace({'past/current':'1'})\n",
    "    \n",
    "    # reveal_risk\n",
    "    df=df.replace({'low risk':'0'})\n",
    "    df=df.replace({'moderately high risk':'1'})\n",
    "    df=df.replace({'high risk':'2'})\n",
    "    df=df.replace({'very high risk':'3'})\n",
    "    \n",
    "    # initial_therapy\n",
    "    df=df.replace({'PDE5':'0'})\n",
    "    df=df.replace({'CCB':'1'})\n",
    "    df=df.replace({'ERA':'2'})\n",
    "    df=df.replace({'PA':'3'})\n",
    "    df=df.replace({'sGC':'4'})\n",
    "    df=df.replace({'combination therapy':'5'})\n",
    "    \n",
    "    #UNK -- NA\n",
    "    # df=df.replace({'UNK':''})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_folder(folder):\n",
    "    import os     \n",
    "    try: \n",
    "        os.mkdir(folder) \n",
    "    except FileExistsError:\n",
    "        print(\"Directory [ %s ] already exists\"%folder)\n",
    "\n",
    "# remove low variance features\n",
    "def remove_low_variance(df, thresh):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    target=df.clusters\n",
    "    thresholder = VarianceThreshold(threshold=thresh)\n",
    "    thresholder.fit_transform(df.drop('clusters',axis=1))\n",
    "    selected_features_mask=thresholder.get_support()\n",
    "    df = df.loc[:,thresholder.get_support(indices=False)] \n",
    "    df['clusters']=target\n",
    "    print(df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def assess_missingness(data):\n",
    "\n",
    "    # pandas series denoting features and the sum of their null values\n",
    "    null_sum = data.isnull().sum()# instantiate columns for missing data\n",
    "    total = null_sum.sort_values(ascending=False)\n",
    "    percent = ( ((null_sum / len(data.index))*100).round(2) ).sort_values(ascending=False)\n",
    "    \n",
    "    # concatenate along the columns to create the complete dataframe\n",
    "    df_NA = pd.concat([total, percent], axis=1, keys=['Number of NA', 'Percent NA'])\n",
    "    \n",
    "    # drop rows that don't have any missing data; may comment out to omit\n",
    "    df_NA = df_NA[ (df_NA.T != 0).any() ]\n",
    "    \n",
    "    return df_NA\n",
    "\n",
    "\n",
    "'''\n",
    "    Take input data 9training and testing\n",
    "    Run Recursive Feature Elimination (RFE)\n",
    "    based on 6 estimators -- SVR, RF, GBR, ETR, LR and XGB\n",
    "'''\n",
    "def run_RFE(X_train, y_train, X_test, y_test, num_to_select):\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.svm import SVR\n",
    "  \n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    \n",
    "    # classifiers\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # classifiers\n",
    "    np.random.seed(220)\n",
    "    \n",
    "    #features \n",
    "    features=X_test.columns\n",
    "    \n",
    "    # feature scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    # estimators -- to get weights and rankings ... \n",
    "    svr = SVC(kernel='linear',random_state=220) # support vector machine\n",
    "    rf = RandomForestClassifier(random_state=220)\n",
    "    gbr = GradientBoostingClassifier(random_state=220,verbosity=0)\n",
    "    logreg = LogisticRegression(random_state=220)\n",
    "    xg = XGBClassifier(random_state=220)\n",
    "    etr = ExtraTreesClassifier(random_state=220)\n",
    "        \n",
    "    # instantiate RFEs\n",
    "    svr_rfe = RFE(svr, num_to_select, step=1, verbose=0)\n",
    "    rf_rfe = RFE(rf, num_to_select, step=1, verbose=0)\n",
    "    gbr_rfe = RFE(gbr, num_to_select, step=1, verbose=0)\n",
    "    logreg_rfe = RFE(logreg, num_to_select, step=1, verbose=0)\n",
    "    xg_rfe = RFE(xg, num_to_select, step=1, verbose=0)\n",
    "    etr_rfe = RFE(etr, num_to_select, step=1, verbose=0)\n",
    "    \n",
    "    # train RFEs\n",
    "    svr_rfe.fit(X_train,y_train); #print('fitting SVR')\n",
    "    rf_rfe.fit(X_train,y_train); #print('fitting RandomforestRegressor')   \n",
    "    gbr_rfe.fit(X_train,y_train); #print('fitting GradientBoostingRegressor')\n",
    "    logreg_rfe.fit(X_train,y_train); #print('fitting LogisticRegression')\n",
    "    xg_rfe.fit(X_train,y_train); #print('fitting XGBoost')\n",
    "    etr_rfe.fit(X_train,y_train); #print('fitting ExtraTreesRegressor')\n",
    "    \n",
    "    # scores\n",
    "    svr_score = svr_rfe.score(X_test,y_test)\n",
    "    rf_score  = rf_rfe.score(X_test,y_test)\n",
    "    gbr_score = gbr_rfe.score(X_test,y_test)\n",
    "    logreg_score = logreg_rfe.score(X_test,y_test)\n",
    "    xg_score = xg_rfe.score(X_test,y_test)\n",
    "    etr_score = etr_rfe.score(X_test,y_test)\n",
    "    \n",
    "    # selected features\n",
    "    rf_feature  = features[rf_rfe.support_]\n",
    "    svr_feature  = features[svr_rfe.support_] # use\n",
    "    gbr_feature = features[gbr_rfe.support_]\n",
    "    logreg_feature = features[logreg_rfe.support_] \n",
    "    xg_feature = features[xg_rfe.support_]\n",
    "    etr_feature = features[etr_rfe.support_]\n",
    "    \n",
    "    # feature rankings\n",
    "    svr_ranking = svr_rfe.ranking_ # use\n",
    "    rf_ranking  = rf_rfe.ranking_  # use\n",
    "    gbr_ranking = gbr_rfe.ranking_ # use\n",
    "    logreg_ranking = logreg_rfe.ranking_ # use\n",
    "    xg_ranking  = xg_rfe.ranking_  # use\n",
    "    etr_ranking = etr_rfe.ranking_ # use\n",
    "    \n",
    "    # selected features from all selectors\n",
    "    selected_features=pd.DataFrame()\n",
    "    selected_features['svr'] = pd.Series(svr_feature)\n",
    "    selected_features['rf'] = pd.Series(rf_feature)\n",
    "    selected_features['xg'] = pd.Series(xg_feature)\n",
    "    selected_features['gbr'] = pd.Series(gbr_feature)\n",
    "    selected_features['etr'] = pd.Series(etr_feature)\n",
    "    selected_features['logreg'] = pd.Series(logreg_feature)\n",
    "    \n",
    "    # feature ranking from all selectors\n",
    "    feature_ranking=pd.DataFrame()\n",
    "    #feature_ranking['features']=pd.Series(features)\n",
    "    feature_ranking['svr'] = pd.Series(svr_rfe.ranking_)\n",
    "    feature_ranking['rf']  = pd.Series(rf_rfe.ranking_)\n",
    "    feature_ranking['xg']  = pd.Series(xg_rfe.ranking_)\n",
    "    feature_ranking['gbr'] = pd.Series(gbr_rfe.ranking_)\n",
    "    feature_ranking['etr'] = pd.Series(etr_rfe.ranking_)\n",
    "    feature_ranking['logreg'] = pd.Series(logreg_rfe.ranking_)\n",
    "    \n",
    "    # scores -- not used if only for feature ranking\n",
    "    feature_ranking_scores=pd.DataFrame()\n",
    "    feature_ranking_scores['svr'] = pd.Series(svr_score)\n",
    "    feature_ranking_scores['rf']  = pd.Series(rf_score)\n",
    "    feature_ranking_scores['xg']  = pd.Series(xg_score)\n",
    "    feature_ranking_scores['gbr'] = pd.Series(gbr_score)\n",
    "    feature_ranking_scores['etr'] = pd.Series(etr_score)\n",
    "    feature_ranking_scores['logreg'] = pd.Series(logreg_score)\n",
    "    \n",
    "    return selected_features, feature_ranking, feature_ranking_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data one dataset -- we have 50\n",
    "df = pd.read_csv('G:/My Drive/ejammeh/Work/Projects/iPAH/Latest/data/imputed dataset 50/imputed_pheno_1.csv',header = 0)\n",
    "\n",
    "# check missingness of raw data\n",
    "#print(df.isnull().sum())\n",
    "\n",
    "# inspect first few rows for every column\n",
    "#print(df.head())\n",
    "\n",
    "# Check variable types\n",
    "#df.info()\n",
    "\n",
    "# basic stats \n",
    "#df.describe()\n",
    "\n",
    "# count cluster memberships\n",
    "print(df.clusters.value_counts())\n",
    "\n",
    "# inspect dataset\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Discovery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to store results\n",
    "create_folder('./results')\n",
    "create_folder('./results/discovery_dataset')\n",
    "create_folder('./results/discovery_dataset/preprocessing')\n",
    "create_folder('./results/discovery_dataset/preprocessing/cleaned')\n",
    "create_folder('./results/discovery_dataset/preprocessing/missingness')\n",
    "\n",
    "# get duplicate featutes : these were identified from previous analysis of the datasets\n",
    "duplicate_features = pd.read_csv('G:/My Drive/ejammeh/Work/Projects/iPAH/Latest/data/duplicate_features.csv', header=0).features\n",
    "\n",
    "# bad features \n",
    "bad_features=np.array(pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/bad_features.csv').features)\n",
    "\n",
    "for i in range (1,51):\n",
    "    # 1: load datasets  \n",
    "    exec(\"df = pd.read_csv('G:/My Drive/ejammeh/Work/Projects/iPAH/Latest/data/imputed dataset 50/imputed_pheno_%d.csv',header = 0)\"%i)\n",
    "\n",
    "    #2: remove duplicate features \n",
    "    df=df.drop(duplicate_features, axis=1)\n",
    "    df=df.drop(columns=bad_features)\n",
    "\n",
    "    # 3: only extract data for clusters A, B & E\n",
    "    df = df[(df['clusters']=='A') | (df['clusters']=='B') | (df['clusters']=='E')]\n",
    "\n",
    "    # 4: identify and remove features with >5% missing values\n",
    "    missing = assess_missingness(df)\n",
    "    missingness=pd.DataFrame(missing)\n",
    "    \n",
    "    '''\n",
    "        4a: Plot missingness -- but only for features that has missing values\n",
    "    '''\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    ax=sns.barplot(data = missingness, x = missingness.index, y = 'Percent NA', ci = None, color='g')\n",
    "    exec(\"plt.title('Discovery Dataset %d Missingness')\"%i)\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    exec(\"plt.savefig('./results/discovery_dataset/preprocessing/missingness/data_%d_missingness.png')\"%i)\n",
    "    exec(\"missingness.to_csv('./results/discovery_dataset/preprocessing/missingness/missingness_%d.csv',index=False)\"%i)\n",
    "    \n",
    "    # no need to dislay all figures\n",
    "    if i > 1:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # 5: drop those features over 5% missingness\n",
    "    features_to_drop = pd.Series(missingness[missingness['Percent NA'] > 5].index) # any feature with 5% missingness to be dropped\n",
    "    df=df.drop(columns = list(features_to_drop)).drop('Unnamed: 0',axis=1)   \n",
    "\n",
    "    # remove some dodgy features\n",
    "    df = df.drop(['id', 'cohort_group', 'diagnosis_verified', 'case', 'reveal_risk', 'cum.haz'], axis=1)\n",
    "    \n",
    "    # 6: encode datasets\n",
    "    df = encode_features_v2(df); \n",
    "\n",
    "    # 7: drop missing values \n",
    "    df=df.dropna()\n",
    "    exec(\"ds_%d\"%i+\" =df.copy()\")\n",
    "    \n",
    "    # 8: save to file\n",
    "    exec(\"df.to_csv('./results/discovery_dataset/preprocessing/cleaned/cleaned_discovery_dataset_%d.csv',index=False)\"%i)\n",
    "\n",
    "# sanity check\n",
    "df.shape\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to store results\n",
    "create_folder('./results')\n",
    "create_folder('./results/validation_dataset')\n",
    "create_folder('./results/validation_dataset/preprocessing')\n",
    "create_folder('./results/validation_dataset/preprocessing/cleaned')\n",
    "create_folder('./results/validation_dataset/preprocessing/missingness')\n",
    "\n",
    "# get duplicate featutes : these were identified from previous analysis of the datasets\n",
    "duplicate_features = pd.read_csv('G:/My Drive/ejammeh/Work/Projects/iPAH/Latest/data/duplicate_features.csv', header=0).features\n",
    "\n",
    "for i in range (1,51):\n",
    "    # 1: load datasets  \n",
    "    exec(\"df = pd.read_csv('G:/My Drive/ejammeh/Work/Projects/iPAH/Latest/data/imputed validation dataset 50/validation_imputed_pheno_%d.csv',header = 0)\"%i)\n",
    "\n",
    "    #2: remove duplicate features \n",
    "    df=df.drop(duplicate_features, axis=1)\n",
    "\n",
    "    # 4: identify and remove features with >5% missing values\n",
    "    missing = assess_missingness(df)\n",
    "    missingness=pd.DataFrame(missing)\n",
    "    \n",
    "    '''\n",
    "        4a: Plot missingness -- but only for features that has missing values\n",
    "    '''\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    ax=sns.barplot(data = missingness, x = missingness.index, y = 'Percent NA', ci = None, color='g')\n",
    "    exec(\"plt.title('Discovery Dataset %d Missingness')\"%i)\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    exec(\"plt.savefig('./results/validation_dataset/preprocessing/missingness/data_%d_missingness.png')\"%i)\n",
    "    exec(\"missingness.to_csv('./results/validation_dataset/preprocessing/missingness/missingness_%d.csv')\"%i)\n",
    "    \n",
    "    # no need to dislay all figures\n",
    "    if i > 1:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # 5: drop those features over 5% missingness\n",
    "    features_to_drop = pd.Series(missingness[missingness['Percent NA'] > 5].index) # any feature with 5% missingness to be dropped\n",
    "    df=df.drop(columns = list(features_to_drop)).drop('Unnamed: 0',axis=1)   \n",
    "\n",
    "    # remove some dodgy features\n",
    "    #df = df.drop(['id', 'cohort_group', 'diagnosis_verified', 'case', 'reveal_risk'], axis=1)\n",
    "    #df = df.drop(['reveal_risk'], axis=1)\n",
    "    \n",
    "    # 6: encode datasets\n",
    "    df = encode_features_v2(df)\n",
    "\n",
    "    # 7: drop missing values \n",
    "    df = df.dropna()\n",
    "    exec(\"dv_%d\"%i+\"=df.copy()\")\n",
    "    \n",
    "    # 8: save to file\n",
    "    exec(\"df.to_csv('./results/validation_dataset/preprocessing/cleaned/cleaned_validation_dataset_%d.csv',index=False)\"%i)\n",
    "    \n",
    "# sanity check\n",
    "df.shape\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Robust feature selection & ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries; create folders; create datasets for each cluster; and setup experiment\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "import statistics as stats\n",
    "from sklearn import metrics\n",
    "from random import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import math\n",
    "\n",
    "# create folders to store results\n",
    "create_folder('./results')\n",
    "create_folder('./results/feature_selection')\n",
    "create_folder('./results/feature_selection/cluster_A')\n",
    "create_folder('./results/feature_selection/cluster_B')\n",
    "create_folder('./results/feature_selection/cluster_E')\n",
    "\n",
    "create_folder('./results/feature_selection/feature_ranking')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_A')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_B')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_E')\n",
    "\n",
    "#number of samples\n",
    "n_repeat = 1000 # number of resampling\n",
    "n_bstraps = 500 # number of bootstraps\n",
    "n_features = 21\n",
    "\n",
    "'''\n",
    "    1: Load the pre-processed and cleaned dataset\n",
    "    2: Create sub datasets around Cluster A, B & E\n",
    "    ** start with just one dataset before scaling to all datasets\n",
    "'''\n",
    "# this should be run for each discovery dataset -- and results aggregrated i.e. the feature rankings\n",
    "ds_1=pd.read_csv('./results/discovery_dataset/preprocessing/cleaned/cleaned_discovery_dataset_1.csv',header=0)\n",
    "df_A=ds_1.copy(); df_A.clusters=df_A.clusters.replace({'A':1, 'B':0, 'C':0, 'D':0, 'E':0})\n",
    "df_B=ds_1.copy(); df_B.clusters=df_B.clusters.replace({'A':0, 'B':1, 'C':0, 'D':0, 'E':0})\n",
    "df_E=ds_1.copy(); df_E.clusters=df_E.clusters.replace({'A':0, 'B':0, 'C':0, 'D':0, 'E':1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3.2. Cluster A feature selection & rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# should add an outer for loop for clusters\n",
    "\n",
    "# Go through Cluter A\n",
    "for size in range (1, n_features): \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        \n",
    "        # grab rankings of each feature based on svr and rfe \n",
    "        exec(\"svr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"rf_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"etr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"xg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    " \n",
    "        # Resampling using 90% of original dataset\n",
    "        sample = resample(df_A, replace=False, n_samples=math.ceil(df_A.shape[0]*0.9), random_state=int((random()*100)))  \n",
    "        \n",
    "        # bootstrap level\n",
    "        for bs in range(1, n_bstraps): \n",
    "            '''\n",
    "                using resample with replacement -- fuul set/size\n",
    "                thus some samples may be repeated\n",
    "            '''\n",
    "            boot = resample(sample, replace=True, n_samples=sample.shape[0], random_state=int((random()*100))) \n",
    "           \n",
    "            '''\n",
    "                extract data and target/clusters\n",
    "                then split dataset into training and testing datasets 70:30\n",
    "            '''\n",
    "            X = boot.drop(columns='clusters') # data\n",
    "            target = boot.clusters #class\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3, stratify=target, random_state=220)\n",
    "            \n",
    "            '''\n",
    "                Then run Recursive Feature Elimination to select a set of features\n",
    "                we use LinearSVC and RandomForestClassifier\n",
    "                \n",
    "            '''\n",
    "            # rfe feature selection -- a number of estimators -- number of features to select is set to one -- just a feature ranking process\n",
    "            selected_features, feature_ranking, feature_ranking_scores = run_RFE(X_train, y_train, X_test, y_test, size)\n",
    "            \n",
    "            if bs==1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d = feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.logreg\"%repeat)\n",
    "            elif bs>1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d += feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.logreg\"%repeat)\n",
    "            \n",
    "            print(\"Robust feature selection: signature size {} sample {} bootstrap {} \".format(size, repeat, bs), end = '\\r')\n",
    "\n",
    "        # aggregrate ranking from all rankers over all bootstrap subsamples\n",
    "        #exec(\"rankings = svr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+rf_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" + xg_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" +gbr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+etr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+ logreg_ranking_%d\"%size+\"_repeat_%d\"%repeat)     \n",
    "        #exec(\"rfe_size_%d_signatures\"%size+\"['rankings_%d']\"%repeat+\"= rankings\")     \n",
    "             \n",
    "# get aggregrate results\n",
    "for size in range (1, n_features): \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        for algo in ['svr','rf','xg','gbr','etr','logreg']:\n",
    "            if repeat==1:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking = %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "            else:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking += %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "    #final rankings\n",
    "    exec(\"final_size_%d\"%size+\"_rankings=pd.DataFrame()\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['features']=pd.Series(X.columns)\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['svr']=overall_svr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['rf']=overall_rf_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['xg']=overall_xg_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['gbr']=overall_gbr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['etr']=overall_etr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['logreg']=overall_logreg_size_%d\"%size+\"_ranking\")\n",
    "    \n",
    "\n",
    "# create folders to store results\n",
    "create_folder('./results')\n",
    "create_folder('./results/feature_selection')\n",
    "create_folder('./results/feature_selection/feature_ranking')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_A')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_B')\n",
    "create_folder('./results/feature_selection/feature_ranking/cluster_E')\n",
    "\n",
    "for size in range (1, n_features): \n",
    "    # get features\n",
    "    exec(\"size_%d\"%size+\"_rankings = pd.DataFrame()\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['features']=pd.Series(X.columns)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['svr']=final_size_%d\"%size+\"_rankings.svr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['rf']=final_size_%d\"%size+\"_rankings.rf\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['xg']=final_size_%d\"%size+\"_rankings.xg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['gbr']=final_size_%d\"%size+\"_rankings.gbr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['etr']=final_size_%d\"%size+\"_rankings.etr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['logreg']=final_size_%d\"%size+\"_rankings.logreg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['average']=size_%d\"%size+\"_rankings.mean(axis=1)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings.sort_values(by='svr', ascending=True, inplace=True)\")\n",
    "\n",
    "    # save to file\n",
    "    exec(\"size_%d\"%size+\"_rankings.to_csv('./results/feature_selection/feature_ranking/cluster_A/cluster_A_size_%d\"%size+\"_feature_rankings_svr.csv')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Cluster B feature selection & rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# should add an outer for loop for clusters\n",
    "\n",
    "# Go through Cluter B\n",
    "for size in range (1, n_features): \n",
    "    \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        \n",
    "        # grab rankings of each feature based on svr and rfe \n",
    "        exec(\"svr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"rf_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"etr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"xg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "\n",
    "        \n",
    "        # Resampling using 90% of original dataset\n",
    "        sample = resample(df_B, replace=False, n_samples=math.ceil(df_B.shape[0]*0.9), random_state=int((random()*100)))  \n",
    "        \n",
    "        # bootstrap level\n",
    "        for bs in range(1, n_bstraps): \n",
    "            '''\n",
    "                using resample with replacement -- fuul set/size\n",
    "                thus some samples may be repeated\n",
    "            '''\n",
    "            boot = resample(sample, replace=True, n_samples=sample.shape[0], random_state=int((random()*100))) \n",
    "           \n",
    "            '''\n",
    "                extract data and target/clusters\n",
    "                then split dataset into training and testing datasets 70:30\n",
    "            '''\n",
    "            X = boot.drop(columns='clusters') # data\n",
    "            target = boot.clusters #class\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3, stratify=target, random_state=220)\n",
    "            \n",
    "            '''\n",
    "                Then run Recursive Feature Elimination to select a set of features\n",
    "                we use LinearSVC and RandomForestClassifier\n",
    "                \n",
    "            '''\n",
    "            # rfe feature selection -- a number of estimators -- number of features to select is set to one -- just a feature ranking process\n",
    "            selected_features, feature_ranking, feature_ranking_scores = run_RFE(X_train, y_train, X_test, y_test, size)\n",
    "            \n",
    "            if bs==1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d = feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.logreg\"%repeat)\n",
    "            elif bs>1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d += feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.logreg\"%repeat)\n",
    "            \n",
    "            print(\"Cluster B -- Robust feature selection: signature size {} sample {} bootstrap {} \".format(size, repeat, bs), end = '\\r')\n",
    "\n",
    "        # aggregrate ranking from all rankers over all bootstrap subsamples\n",
    "        #exec(\"rankings = svr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+rf_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" + xg_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" +gbr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+etr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+ logreg_ranking_%d\"%size+\"_repeat_%d\"%repeat)     \n",
    "        #exec(\"rfe_size_%d_signatures\"%size+\"['rankings_%d']\"%repeat+\"= rankings\")     \n",
    "\n",
    "             \n",
    "# get aggregrate results\n",
    "for size in range (1, n_features): \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        for algo in ['svr','rf','xg','gbr','etr','logreg']:\n",
    "            if repeat==1:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking = %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "            else:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking += %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "    #final rankings\n",
    "    exec(\"final_size_%d\"%size+\"_rankings=pd.DataFrame()\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['features']=pd.Series(X.columns)\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['svr']=overall_svr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['rf']=overall_rf_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['xg']=overall_xg_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['gbr']=overall_gbr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['etr']=overall_etr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['logreg']=overall_logreg_size_%d\"%size+\"_ranking\")\n",
    "    \n",
    "\n",
    "for size in range (1, n_features): \n",
    "    # get features\n",
    "    exec(\"size_%d\"%size+\"_rankings = pd.DataFrame()\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['features']=pd.Series(X.columns)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['svr']=final_size_%d\"%size+\"_rankings.svr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['rf']=final_size_%d\"%size+\"_rankings.rf\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['xg']=final_size_%d\"%size+\"_rankings.xg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['gbr']=final_size_%d\"%size+\"_rankings.gbr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['etr']=final_size_%d\"%size+\"_rankings.etr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['logreg']=final_size_%d\"%size+\"_rankings.logreg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['average']=size_%d\"%size+\"_rankings.mean(axis=1)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings.sort_values(by='svr', ascending=True, inplace=True)\")\n",
    "\n",
    "    # save to file\n",
    "    exec(\"size_%d\"%size+\"_rankings.to_csv('./results/feature_selection/feature_ranking/cluster_B/cluster_B_size_%d\"%size+\"_feature_rankings_svr.csv')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Cluster E feature selection & rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# should add an outer for loop for clusters\n",
    "\n",
    "# Go through Cluter E\n",
    "for size in range (1, n_features): \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        \n",
    "        # grab rankings of each feature based on svr and rfe \n",
    "        exec(\"svr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"rf_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"etr_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "        exec(\"xg_ranking_%d\"%size+\"_repeat_%d = []\"%repeat)\n",
    "\n",
    "        \n",
    "        # Resampling using 90% of original dataset\n",
    "        sample = resample(df_E, replace=False, n_samples=math.ceil(df_E.shape[0]*0.9), random_state=int((random()*100)))  \n",
    "        \n",
    "        # bootstrap level\n",
    "        for bs in range(1, n_bstraps): \n",
    "            '''\n",
    "                using resample with replacement -- fuul set/size\n",
    "                thus some samples may be repeated\n",
    "            '''\n",
    "            boot = resample(sample, replace=True, n_samples=sample.shape[0], random_state=int((random()*100))) \n",
    "           \n",
    "            '''\n",
    "                extract data and target/clusters\n",
    "                then split dataset into training and testing datasets 70:30\n",
    "            '''\n",
    "            X = boot.drop(columns='clusters') # data\n",
    "            target = boot.clusters #class\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3, stratify=target, random_state=220)\n",
    "            \n",
    "            '''\n",
    "                Then run Recursive Feature Elimination to select a set of features\n",
    "                we use LinearSVC and RandomForestClassifier\n",
    "                \n",
    "            '''\n",
    "            # rfe feature selection -- a number of estimators -- number of features to select is set to one -- just a feature ranking process\n",
    "            selected_features, feature_ranking, feature_ranking_scores = run_RFE(X_train, y_train, X_test, y_test, size)\n",
    "            \n",
    "            if bs==1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d = feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d = feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d = feature_ranking.logreg\"%repeat)\n",
    "            elif bs>1:\n",
    "                exec(\"svr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.svr\"%repeat)\n",
    "                exec(\"rf_ranking_%d\"%size+\"_repeat_%d += feature_ranking.rf\"%repeat)\n",
    "                exec(\"xg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.xg\"%repeat)\n",
    "                exec(\"gbr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.gbr\"%repeat)\n",
    "                exec(\"etr_ranking_%d\"%size+\"_repeat_%d += feature_ranking.etr\"%repeat)\n",
    "                exec(\"logreg_ranking_%d\"%size+\"_repeat_%d += feature_ranking.logreg\"%repeat)\n",
    "            \n",
    "            print(\"Cluster E -- Robust feature selection: signature size {} sample {} bootstrap {} \".format(size, repeat, bs), end = '\\r')\n",
    "\n",
    "        # aggregrate ranking from all rankers over all bootstrap subsamples\n",
    "        #exec(\"rankings = svr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+rf_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" + xg_ranking_%d\"%size+\"_repeat_%d\"%repeat+\" +gbr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+etr_ranking_%d\"%size+\"_repeat_%d\"%repeat+\"+ logreg_ranking_%d\"%size+\"_repeat_%d\"%repeat)     \n",
    "        #exec(\"rfe_size_%d_signatures\"%size+\"['rankings_%d']\"%repeat+\"= rankings\")     \n",
    "         \n",
    "# get aggregrate results\n",
    "for size in range (1, n_features): \n",
    "    for repeat in range(1,n_repeat, 1): \n",
    "        for algo in ['svr','rf','xg','gbr','etr','logreg']:\n",
    "            if repeat==1:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking = %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "            else:\n",
    "                exec(\"overall_%s\"%algo+\"_size_%d\"%size+\"_ranking += %s\"%algo+\"_ranking_%d\"%size+\"_repeat_%d\"%repeat)\n",
    "    #final rankings\n",
    "    exec(\"final_size_%d\"%size+\"_rankings=pd.DataFrame()\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['features']=pd.Series(X.columns)\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['svr']=overall_svr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['rf']=overall_rf_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['xg']=overall_xg_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['gbr']=overall_gbr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['etr']=overall_etr_size_%d\"%size+\"_ranking\")\n",
    "    exec(\"final_size_%d\"%size+\"_rankings['logreg']=overall_logreg_size_%d\"%size+\"_ranking\")\n",
    "    \n",
    "for size in range (1, n_features): \n",
    "    # get features\n",
    "    exec(\"size_%d\"%size+\"_rankings = pd.DataFrame()\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['features']=pd.Series(X.columns)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['svr']=final_size_%d\"%size+\"_rankings.svr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['rf']=final_size_%d\"%size+\"_rankings.rf\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['xg']=final_size_%d\"%size+\"_rankings.xg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['gbr']=final_size_%d\"%size+\"_rankings.gbr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['etr']=final_size_%d\"%size+\"_rankings.etr\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['logreg']=final_size_%d\"%size+\"_rankings.logreg\")\n",
    "    exec(\"size_%d\"%size+\"_rankings ['average']=size_%d\"%size+\"_rankings.mean(axis=1)\")\n",
    "    exec(\"size_%d\"%size+\"_rankings.sort_values(by='svr', ascending=True, inplace=True)\")\n",
    "\n",
    "    # save to file\n",
    "    exec(\"size_%d\"%size+\"_rankings.to_csv('./results/feature_selection/feature_ranking/cluster_E/cluster_E_size_%d\"%size+\"_feature_rankings_svr.csv')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Cluster A classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Use signature A for classification\n",
    "for algo in ['svr', 'svc', 'lasso', 'lr', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "    exec(\"%s\"%algo+\"_train_accy = []\")\n",
    "    exec(\"%s\"%algo+\"_test_accy = []\")\n",
    "\n",
    "for size in range (1, n_features): \n",
    "    # 1: get signature\n",
    "    #exec(\"features=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_A/cluster_A_size_%d\"%size+\"_feature_rankings_svr.csv',header=0).features[0:%d]\"%size)\n",
    "    exec(\"tmp=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_A/cluster_A_size_%d\"%size+\"_feature_rankings_svr.csv',header=0)\")\n",
    "    tmp.sort_values(by='average', ascending=True, inplace=True)\n",
    "    exec(\"features=tmp.features[0:%d]\"%size)\n",
    "    \n",
    "    # 2: filter data based on selected signature\n",
    "    X=df_A[features]\n",
    "    y=df_A.clusters\n",
    "    \n",
    "    # 3: Split the dataset into  Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 220)\n",
    "    \n",
    "    # 4: data scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    #X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # 5: classifiers creation and fitting\n",
    "    svr = LinearSVC(random_state=220).fit(X_train, y_train)  # Linear SVC\n",
    "    svc = SVC(kernel='rbf',random_state=220).fit(X_train, y_train)  # SCV\n",
    "    lasso = Lasso(alpha=1e-6,normalize=True, max_iter=1e5, random_state=220).fit(X_train, y_train) # Lasso\n",
    "    lr = LogisticRegression(n_jobs = -1, random_state=220).fit(X_train, y_train) # Log\n",
    "    dt = DecisionTreeClassifier(random_state=220).fit(X_train, y_train)\n",
    "    knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    lda = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "    gnb = GaussianNB().fit(X_train, y_train)\n",
    "    rf = RandomForestClassifier(random_state=220).fit(X_train, y_train)\n",
    "    \n",
    "    # classifiers able to get feature weights -- linear\n",
    "    for algo in ['svr', 'lasso', 'lr']:\n",
    "        exec(\"%s\"%algo+\"_weights=pd.Series(np.ravel(%s\"%algo+\".coef_))\")\n",
    "        exec(\"%s\"%algo+\"_weights = %s\"%algo+\"_weights/np.max(abs(%s\"%algo+\"_weights))\")\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "    # classifiers unable to get feature weights \n",
    "    for algo in ['svc', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "# save results\n",
    "cluster_A_train_clf_res=pd.DataFrame()\n",
    "cluster_A_train_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_A_train_clf_res['%s\"%algo+\"']=%s\"%algo+\"_train_accy\")\n",
    "    \n",
    "# save results\n",
    "cluster_A_test_clf_res=pd.DataFrame()\n",
    "cluster_A_test_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_A_test_clf_res['%s\"%algo+\"']=%s\"%algo+\"_test_accy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Cluster B classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Use signature B for classification\n",
    "for algo in ['svr', 'svc', 'lasso', 'lr', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "    exec(\"%s\"%algo+\"_train_accy = []\")\n",
    "    exec(\"%s\"%algo+\"_test_accy = []\")\n",
    "\n",
    "for size in range (1, n_features): \n",
    "    # 1: get signature\n",
    "    #exec(\"features=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_A/cluster_A_size_%d\"%size+\"_feature_rankings_svr.csv',header=0).features[0:%d]\"%size)\n",
    "    exec(\"tmp=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_B/cluster_B_size_%d\"%size+\"_feature_rankings_svr.csv',header=0)\")\n",
    "    tmp.sort_values(by='average', ascending=True, inplace=True)\n",
    "    exec(\"features=tmp.features[0:%d]\"%size)\n",
    "    \n",
    "    # 2: filter data based on selected signature\n",
    "    X=df_B[features]\n",
    "    y=df_B.clusters\n",
    "    \n",
    "    # 3: Split the dataset into  Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 220)\n",
    "    \n",
    "    # 4: data scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    #X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # 5: classifiers creation and fitting\n",
    "    svr = LinearSVC(random_state=220).fit(X_train, y_train)  # Linear SVC\n",
    "    svc = SVC(kernel='rbf',random_state=220).fit(X_train, y_train)  # SCV\n",
    "    lasso = Lasso(alpha=1e-6,normalize=True, max_iter=1e5, random_state=220).fit(X_train, y_train) # Lasso\n",
    "    lr = LogisticRegression(n_jobs = -1, random_state=220).fit(X_train, y_train) # Log\n",
    "    dt = DecisionTreeClassifier(random_state=220).fit(X_train, y_train)\n",
    "    knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    lda = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "    gnb = GaussianNB().fit(X_train, y_train)\n",
    "    rf = RandomForestClassifier(random_state=220).fit(X_train, y_train)\n",
    "    \n",
    "    # classifiers able to get feature weights -- linear\n",
    "    for algo in ['svr', 'lasso', 'lr']:\n",
    "        exec(\"%s\"%algo+\"_weights=pd.Series(np.ravel(%s\"%algo+\".coef_))\")\n",
    "        exec(\"%s\"%algo+\"_weights = %s\"%algo+\"_weights/np.max(abs(%s\"%algo+\"_weights))\")\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "    # classifiers unable to get feature weights \n",
    "    for algo in ['svc', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "# save results\n",
    "cluster_B_train_clf_res=pd.DataFrame()\n",
    "cluster_B_train_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_B_train_clf_res['%s\"%algo+\"']=%s\"%algo+\"_train_accy\")\n",
    "    \n",
    "# save results\n",
    "cluster_B_test_clf_res=pd.DataFrame()\n",
    "cluster_B_test_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_B_test_clf_res['%s\"%algo+\"']=%s\"%algo+\"_test_accy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Cluster E classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Use signature B for classification\n",
    "for algo in ['svr', 'svc', 'lasso', 'lr', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "    exec(\"%s\"%algo+\"_train_accy = []\")\n",
    "    exec(\"%s\"%algo+\"_test_accy = []\")\n",
    "\n",
    "for size in range (1, n_features): \n",
    "    # 1: get signature\n",
    "    #exec(\"features=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_A/cluster_A_size_%d\"%size+\"_feature_rankings_svr.csv',header=0).features[0:%d]\"%size)\n",
    "    exec(\"tmp=pd.read_csv('G:/My Drive/ejammeh/Work/ipah_feature_selection/robust_feature_selection/code/results/feature_selection/feature_ranking/cluster_E/cluster_E_size_%d\"%size+\"_feature_rankings_svr.csv',header=0)\")\n",
    "    tmp.sort_values(by='average', ascending=True, inplace=True)\n",
    "    exec(\"features=tmp.features[0:%d]\"%size)\n",
    "    \n",
    "    # 2: filter data based on selected signature\n",
    "    X=df_E[features]\n",
    "    y=df_E.clusters\n",
    "    \n",
    "    # 3: Split the dataset into  Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 220)\n",
    "    \n",
    "    # 4: data scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # 5: classifiers creation and fitting\n",
    "    svr = LinearSVC(random_state=220).fit(X_train, y_train)  # Linear SVC\n",
    "    svc = SVC(kernel='rbf',random_state=220).fit(X_train, y_train)  # SCV\n",
    "    lasso = Lasso(alpha=1e-6,normalize=True, max_iter=1e5, random_state=220).fit(X_train, y_train) # Lasso\n",
    "    lr = LogisticRegression(n_jobs = -1, random_state=220).fit(X_train, y_train) # Log\n",
    "    dt = DecisionTreeClassifier(random_state=220).fit(X_train, y_train)\n",
    "    knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    lda = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "    gnb = GaussianNB().fit(X_train, y_train)\n",
    "    rf = RandomForestClassifier(random_state=220).fit(X_train, y_train)\n",
    "    \n",
    "    # classifiers able to get feature weights -- linear\n",
    "    for algo in ['svr', 'lasso', 'lr']:\n",
    "        exec(\"%s\"%algo+\"_weights=pd.Series(np.ravel(%s\"%algo+\".coef_))\")\n",
    "        exec(\"%s\"%algo+\"_weights = %s\"%algo+\"_weights/np.max(abs(%s\"%algo+\"_weights))\")\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "    # classifiers unable to get feature weights \n",
    "    for algo in ['svc', 'dt', 'knn', 'lda', 'gnb', 'rf']:\n",
    "        exec(\"%s\"%algo+\"_train_pred = %s\"%algo+\".predict(X_train)\")\n",
    "        exec(\"%s\"%algo+\"_test_pred = %s\"%algo+\".predict(X_test)\")\n",
    "        exec(\"%s\"%algo+\"_test_acc = round(%s\"%algo+\".score(X_test, y_test) * 100, 2)\")\n",
    "        exec(\"%s\"%algo+\"_train_acc = round(%s\"%algo+\".score(X_train, y_train) * 100, 2)\")           \n",
    "        exec(\"%s\"%algo+\"_train_accy.append(%s\"%algo+\"_train_acc)\")\n",
    "        exec(\"%s\"%algo+\"_test_accy.append(%s\"%algo+\"_test_acc)\")\n",
    "    \n",
    "# save results\n",
    "cluster_E_train_clf_res=pd.DataFrame()\n",
    "cluster_E_train_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_E_train_clf_res['%s\"%algo+\"']=%s\"%algo+\"_train_accy\")\n",
    "    \n",
    "# save results\n",
    "cluster_E_test_clf_res=pd.DataFrame()\n",
    "cluster_E_test_clf_res['size']=range(1,21)\n",
    "for algo in ['svr', 'lr', 'knn', 'rf']:\n",
    "    exec(\"cluster_E_test_clf_res['%s\"%algo+\"']=%s\"%algo+\"_test_accy\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
